{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos\n",
    "\n",
    "Este módulo foi dedicado ao modelo, tentamos com o Naive Bayers, o KNN, Floresta Randomica, Gradient Boosting e o Voting Classifier\n",
    "\n",
    "\n",
    "Carregamos os dados e dividimos as classes em X e y, colocadas dentro de um numpy.array, para serem entradas dos modelos.\n",
    "\n",
    "O Naive Bayes, foi escrito em um código rápido para uma tentativa com a competição, o resultado é foi inicialmente um score de 0.14 porque esquecemos de reconverter a saída. Após reconvertermos conseguimos 0.69906, todavia os dados como podemos observar nos gráficos eles não são normais, e também não normalizamos, decidimos descartar e refazer todos os modelos. \n",
    "\n",
    "De inicio pensamos em remover o TUE, CH2O e o FAF, testamos as submissões com as alterações também, todavia não houve melhoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('df_transformado.pkl')\n",
    "df_test = pd.read_pickle('df_test_transformado.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__Gender_Female</th>\n",
       "      <th>__Gender_Male</th>\n",
       "      <th>id</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>family_history_with_overweight</th>\n",
       "      <th>FAVC</th>\n",
       "      <th>FCVC</th>\n",
       "      <th>NCP</th>\n",
       "      <th>CAEC</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>CH2O</th>\n",
       "      <th>SCC</th>\n",
       "      <th>FAF</th>\n",
       "      <th>TUE</th>\n",
       "      <th>CALC</th>\n",
       "      <th>NObeyesdad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>1.699998</td>\n",
       "      <td>81.66995</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.763573</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.976473</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>1.71146</td>\n",
       "      <td>50.165754</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.910378</td>\n",
       "      <td>1</td>\n",
       "      <td>0.866045</td>\n",
       "      <td>1.673584</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>1.71073</td>\n",
       "      <td>131.274851</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.674061</td>\n",
       "      <td>1</td>\n",
       "      <td>1.467863</td>\n",
       "      <td>0.780199</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>1.914186</td>\n",
       "      <td>93.798055</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.979848</td>\n",
       "      <td>1</td>\n",
       "      <td>1.967973</td>\n",
       "      <td>0.931721</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  __Gender_Female __Gender_Male id  Age    Height      Weight  \\\n",
       "0             0.0           1.0  0   24  1.699998    81.66995   \n",
       "1             1.0           0.0  1   18      1.56        57.0   \n",
       "2             1.0           0.0  2   18   1.71146   50.165754   \n",
       "3             1.0           0.0  3   21   1.71073  131.274851   \n",
       "4             0.0           1.0  4   32  1.914186   93.798055   \n",
       "\n",
       "   family_history_with_overweight  FAVC  FCVC  NCP  CAEC  SMOKE      CH2O  \\\n",
       "0                               1     1     2    3     1      0  2.763573   \n",
       "1                               1     1     2    3     2      0       2.0   \n",
       "2                               1     1     2    1     1      0  1.910378   \n",
       "3                               1     1     3    3     1      0  1.674061   \n",
       "4                               1     1     3    2     1      0  1.979848   \n",
       "\n",
       "   SCC       FAF       TUE  CALC  NObeyesdad  \n",
       "0    1       0.0  0.976473     1           3  \n",
       "1    1       1.0       1.0     0           1  \n",
       "2    1  0.866045  1.673584     0           0  \n",
       "3    1  1.467863  0.780199     1           6  \n",
       "4    1  1.967973  0.931721     1           3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['NObeyesdad']\n",
    "id = df['id']\n",
    "X = df.drop(['NObeyesdad','id'], axis=1)\n",
    "\n",
    "y = y.to_numpy()\n",
    "X = X.to_numpy()\n",
    "\n",
    "id_test = df_test['id']\n",
    "X_test = df_test.drop(['id'],axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobsedy_invertio = {3: 'Overweight_Level_II', 1: 'Normal_Weight', 0: 'Insufficient_Weight',\n",
    "                     6: 'Obesity_Type_III', 5: 'Obesity_Type_II', 2: 'Overweight_Level_I', 4: 'Obesity_Type_I'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolhemos o KNN por ser um clássico, fácil comprensão, usamos o kfold para avaliar as metricas ao redor de cada conjunto do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\larae\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:110: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\larae\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 217, in _count_physical_cores\n",
      "    raise ValueError(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0> accuracy =  0.8559730250481695\n",
      "fold 1> accuracy =  0.8526011560693642\n",
      "fold 2> accuracy =  0.8482658959537572\n",
      "fold 3> accuracy =  0.8554913294797688\n",
      "fold 4> accuracy =  0.8434489402697495\n",
      "fold 5> accuracy =  0.8453757225433526\n",
      "fold 6> accuracy =  0.8574181117533719\n",
      "fold 7> accuracy =  0.8492292870905588\n",
      "fold 8> accuracy =  0.860722891566265\n",
      "fold 9> accuracy =  0.8578313253012049\n",
      "Accuracy Mean: 0.853, Standard Deviation: 0.005\n"
     ]
    }
   ],
   "source": [
    "# k-fold cross validation\n",
    "scores = list()\n",
    "kfold = KFold(10, shuffle=True, random_state=4)\n",
    "\n",
    "for k, (train, test) in enumerate(kfold.split(X, y)):\n",
    "\t# get data\n",
    "\ttrain_X, test_X = X[train], X[test]\n",
    "\ttrain_y, test_y = y[train], y[test]\n",
    "\t# fit model\n",
    "\tmodel = KNeighborsClassifier(n_neighbors=10)\n",
    "\tmodel.fit(train_X, train_y)\n",
    "\t# evaluate model\n",
    "\typred = model.predict(test_X)\n",
    "\tacc = accuracy_score(test_y, ypred)\n",
    "\t# store score\n",
    "\tscores.append(acc)\n",
    "\tprint(f'fold {k}> accuracy = ', acc)\n",
    "\n",
    "# summarize model performance\n",
    "mean_s, std_s = np.mean(scores), np.std(scores)\n",
    "print('Accuracy Mean: %.3f, Standard Deviation: %.3f' % (mean_s, std_s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que ele tem uma queda de acuracia do fold 2, no fold 4 e no fold 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Floresta de Árvore de Decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0> accuracy =  0.9070327552986512\n",
      "fold 1> accuracy =  0.8959537572254336\n",
      "fold 2> accuracy =  0.8892100192678227\n",
      "fold 3> accuracy =  0.8935452793834296\n",
      "fold 4> accuracy =  0.8959537572254336\n",
      "fold 5> accuracy =  0.9041425818882466\n",
      "fold 6> accuracy =  0.9060693641618497\n",
      "fold 7> accuracy =  0.8925818882466281\n",
      "fold 8> accuracy =  0.9108433734939759\n",
      "fold 9> accuracy =  0.896867469879518\n",
      "Accuracy Mean: 0.899, Standard Deviation: 0.007\n"
     ]
    }
   ],
   "source": [
    "# k-fold cross validation\n",
    "scores = list()\n",
    "kfold = KFold(10, shuffle=True, random_state=4)\n",
    "\n",
    "for k, (train, test) in enumerate(kfold.split(X, y)):\n",
    "\t# get data\n",
    "\ttrain_X, test_X = X[train], X[test]\n",
    "\ttrain_y, test_y = y[train], y[test]\n",
    "\t# fit model\n",
    "\tmodel = RandomForestClassifier(n_estimators=300)\n",
    "\tmodel.fit(train_X, train_y)\n",
    "\t# evaluate model\n",
    "\typred = model.predict(test_X)\n",
    "\tacc = accuracy_score(test_y, ypred)\n",
    "\t# store score\n",
    "\tscores.append(acc)\n",
    "\tprint(f'fold {k}> accuracy = ', acc)\n",
    "\n",
    "# summarize model performance\n",
    "mean_s, std_s = np.mean(scores), np.std(scores)\n",
    "print('Accuracy Mean: %.3f, Standard Deviation: %.3f' % (mean_s, std_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=300)\n",
    "rf_model.fit(X,y)\n",
    "y_pred = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random_forest = pd.DataFrame({'id': id_test, 'NObeyesdad': y_pred})\n",
    "df_random_forest['NObeyesdad']  = df_random_forest['NObeyesdad'].map(nobsedy_invertio)\n",
    "\n",
    "df_random_forest.to_csv('submtions/randomforest.csv', index=False, sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A floresta revelou os problemas nos mesmos folds, destancando dessa vez o fold 9, que também tinha um valor mais baixo no knn. Mas se saiu melhor para o conjunto de dados. O estimador foi definido como n=300 porque foi o melhor valor encontrado nos testes, igual ao critério, que no caso por padrão é o gini, fizemos com log_loss mas não houve mudanças significativas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos vasculhando a documentação do skitlern, decidimos testar os dados com outro altoritmo, escolhemos algum que nunca tinhamos testado. Tentamos com além do Gradient Boost, com a Regressão Logistica e as Maquinas de Suporte Vetoriais, todavia os algoritmos não convergiam, então decidimos manter com o Gradient Boost. O algoritmo foi testado de três formas, removendo CH2O, TUE e FAF, teve perdas pequenas no treinamento. Ao usar n_estimator=300, o treinamento teve bons resultados, todavia a acuracia caiu no teste.\n",
    "\n",
    "O algoritmo foi colocado no Kaggle sem TUE E FAF, atingiu 0.89559, com esses 0.89776, não fizemos o teste sem CH2O. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0> accuracy =  0.9104046242774566\n",
      "fold 1> accuracy =  0.901252408477842\n",
      "fold 2> accuracy =  0.8921001926782274\n",
      "fold 3> accuracy =  0.8988439306358381\n",
      "fold 4> accuracy =  0.9051059730250481\n",
      "fold 5> accuracy =  0.9065510597302505\n",
      "fold 6> accuracy =  0.9075144508670521\n",
      "fold 7> accuracy =  0.8945086705202312\n",
      "fold 8> accuracy =  0.9074698795180723\n",
      "fold 9> accuracy =  0.9069879518072289\n",
      "Accuracy Mean: 0.903, Standard Deviation: 0.006\n"
     ]
    }
   ],
   "source": [
    "scores = list()\n",
    "kfold = KFold(10, shuffle=True, random_state=4)\n",
    "\n",
    "for k, (train, test) in enumerate(kfold.split(X, y)):\n",
    "\t# get data\n",
    "\ttrain_X, test_X = X[train], X[test]\n",
    "\ttrain_y, test_y = y[train], y[test]\n",
    "\t# fit model\n",
    "\tmodel =  GradientBoostingClassifier()\n",
    "\tmodel.fit(train_X, train_y)\n",
    "\t# evaluate model\n",
    "\typred = model.predict(test_X)\n",
    "\tacc = accuracy_score(test_y, ypred)\n",
    "\t# store score\n",
    "\tscores.append(acc)\n",
    "\tprint(f'fold {k}> accuracy = ', acc)\n",
    "\n",
    "# summarize model performance\n",
    "mean_s, std_s = np.mean(scores), np.std(scores)\n",
    "print('Accuracy Mean: %.3f, Standard Deviation: %.3f' % (mean_s, std_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier()\n",
    "gb_model.fit(X,y)\n",
    "y_pred = gb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gradient_boosting = pd.DataFrame({'id': id_test, 'NObeyesdad': y_pred})\n",
    "df_gradient_boosting['NObeyesdad']  = df_gradient_boosting['NObeyesdad'].map(nobsedy_invertio)\n",
    "\n",
    "df_gradient_boosting.to_csv('submtions/gradient_boosting.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo foi usado após  olhar exemplos de aplicações da Floresta de Árvores e do Gradient Boosting. Achamos interessantes pois é um cômite de cômite. Pensamos que poderiamos usar porque como observamos a cima, temos perdas de desempenho em alguns folds, então se usamos um cômite de cômite, talvez consegueriamos resultados melhores, onde o fold se saiu melhor. Embora tenha aumentado a acurácia, não aumentou como esperavamos porque precisamos encontrar valores altos no folds de menores acurácias. Mas foi aqui que conseguimos o primeiro valor acima de 0.9 no Kaggle, no caso 0.90137."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0> accuracy =  0.9161849710982659\n",
      "fold 1> accuracy =  0.9060693641618497\n",
      "fold 2> accuracy =  0.8959537572254336\n",
      "fold 3> accuracy =  0.899325626204239\n",
      "fold 4> accuracy =  0.905587668593449\n",
      "fold 5> accuracy =  0.9089595375722543\n",
      "fold 6> accuracy =  0.9137764932562621\n",
      "fold 7> accuracy =  0.896917148362235\n",
      "fold 8> accuracy =  0.9108433734939759\n",
      "fold 9> accuracy =  0.9137349397590362\n",
      "Accuracy Mean: 0.907, Standard Deviation: 0.007\n"
     ]
    }
   ],
   "source": [
    "scores = list()\n",
    "kfold = KFold(10, shuffle=True, random_state=4)\n",
    "\n",
    "for k, (train, test) in enumerate(kfold.split(X, y)):\n",
    "\t# get data\n",
    "\ttrain_X, test_X = X[train], X[test]\n",
    "\ttrain_y, test_y = y[train], y[test]\n",
    "\t# fit model\n",
    "\tclf2 = RandomForestClassifier(n_estimators=300, random_state=1)\n",
    "\tclf1 = GradientBoostingClassifier()\n",
    "\tclf3 = GradientBoostingClassifier(n_estimators=300)\n",
    "\tmodel = VotingClassifier( estimators=[('gdb', clf1), ('rf', clf2), ('gdb300', clf3)],  voting='hard')\n",
    "\tmodel.fit(train_X, train_y)\n",
    "\t# evaluate model\n",
    "\typred = model.predict(test_X)\n",
    "\tacc = accuracy_score(test_y, ypred)\n",
    "\t# store score\n",
    "\tscores.append(acc)\n",
    "\tprint(f'fold {k}> accuracy = ', acc)\n",
    "\n",
    "# summarize model performance\n",
    "mean_s, std_s = np.mean(scores), np.std(scores)\n",
    "print('Accuracy Mean: %.3f, Standard Deviation: %.3f' % (mean_s, std_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = RandomForestClassifier(n_estimators=300, random_state=1)\n",
    "clf1 = GradientBoostingClassifier()\n",
    "clf3 = GradientBoostingClassifier(n_estimators=300)\n",
    "model = VotingClassifier( estimators=[('gdb', clf1), ('rf', clf2), ('gdb300', clf3)],  voting='hard')\n",
    "model.fit(train_X, train_y)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voting = pd.DataFrame({'id': id_test, 'NObeyesdad': y_pred})\n",
    "df_voting['NObeyesdad']  = df_voting['NObeyesdad'].map(nobsedy_invertio)\n",
    "\n",
    "df_voting.to_csv('submtions/voting.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguindo a documentação do Skitlern que nos recomendou o ExtraTreeClassifier é um classificador muito mais aleatório que o RandomForest porque ao construir cada árvore, são consideradas mais divisões aleatórias do que no caso de uma floresta aleatória tradicional. Isso pode levar a modelos mais robustos em algumas situações, especialmente quando há ruído nos dados. Considerando que os dados estão meio estranhos, vistos que os valores que deveriam ser discretos, vieram como continuos e não tem muito o que podemos fazer porque o gerador de dados sintéticos os tornou dessa maneira. \n",
    "\n",
    "Pensamos em usar o ExtraTreesClassifier para identificar atributos com problemas e tentar novamente remove-los."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03621541, 0.06093127, 0.09084781, 0.0852309 , 0.3300857 ,\n",
       "       0.04625147, 0.01701335, 0.05633342, 0.03656436, 0.03354726,\n",
       "       0.00204393, 0.05335283, 0.00816784, 0.05112094, 0.05246689,\n",
       "       0.03982662])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(X, y)\n",
    "clf.feature_importances_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embora não esteja tão didático acima, os atributos com menores valores foram SMOKE, SCC e FCVC. Bem distantes do que estavamos questionando anteriormente, fizemos o teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__Gender_Female</th>\n",
       "      <th>__Gender_Male</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>family_history_with_overweight</th>\n",
       "      <th>FAVC</th>\n",
       "      <th>FCVC</th>\n",
       "      <th>NCP</th>\n",
       "      <th>CAEC</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>CH2O</th>\n",
       "      <th>SCC</th>\n",
       "      <th>FAF</th>\n",
       "      <th>TUE</th>\n",
       "      <th>CALC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24</td>\n",
       "      <td>1.699998</td>\n",
       "      <td>81.66995</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.763573</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.976473</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1.71146</td>\n",
       "      <td>50.165754</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.910378</td>\n",
       "      <td>1</td>\n",
       "      <td>0.866045</td>\n",
       "      <td>1.673584</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21</td>\n",
       "      <td>1.71073</td>\n",
       "      <td>131.274851</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.674061</td>\n",
       "      <td>1</td>\n",
       "      <td>1.467863</td>\n",
       "      <td>0.780199</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "      <td>1.914186</td>\n",
       "      <td>93.798055</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.979848</td>\n",
       "      <td>1</td>\n",
       "      <td>1.967973</td>\n",
       "      <td>0.931721</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  __Gender_Female __Gender_Male  Age    Height      Weight  \\\n",
       "0             0.0           1.0   24  1.699998    81.66995   \n",
       "1             1.0           0.0   18      1.56        57.0   \n",
       "2             1.0           0.0   18   1.71146   50.165754   \n",
       "3             1.0           0.0   21   1.71073  131.274851   \n",
       "4             0.0           1.0   32  1.914186   93.798055   \n",
       "\n",
       "   family_history_with_overweight  FAVC  FCVC  NCP  CAEC  SMOKE      CH2O  \\\n",
       "0                               1     1     2    3     1      0  2.763573   \n",
       "1                               1     1     2    3     2      0       2.0   \n",
       "2                               1     1     2    1     1      0  1.910378   \n",
       "3                               1     1     3    3     1      0  1.674061   \n",
       "4                               1     1     3    2     1      0  1.979848   \n",
       "\n",
       "   SCC       FAF       TUE  CALC  \n",
       "0    1       0.0  0.976473     1  \n",
       "1    1       1.0       1.0     0  \n",
       "2    1  0.866045  1.673584     0  \n",
       "3    1  1.467863  0.780199     1  \n",
       "4    1  1.967973  0.931721     1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exemplo = df.drop(['id','NObeyesdad'],axis=1)\n",
    "exemplo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = (df.drop(['NObeyesdad','id','SMOKE','SCC','FCVC'], axis=1)).to_numpy()\n",
    "X2_test = (df_test.drop(['id','SMOKE','SCC','FCVC'], axis=1)).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest 2 e GrandientBoost 2\n",
    "\n",
    "Não testamos com o KNN acreditamos que a forma como estão os dados, no momento favorece os comites em relação ao classico KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0> accuracy =  0.9070327552986512\n",
      "fold 1> accuracy =  0.8892100192678227\n",
      "fold 2> accuracy =  0.8921001926782274\n",
      "fold 3> accuracy =  0.8892100192678227\n",
      "fold 4> accuracy =  0.8921001926782274\n",
      "fold 5> accuracy =  0.899325626204239\n",
      "fold 6> accuracy =  0.9026974951830443\n",
      "fold 7> accuracy =  0.8872832369942196\n",
      "fold 8> accuracy =  0.9040963855421686\n",
      "fold 9> accuracy =  0.8987951807228916\n",
      "Accuracy Mean: 0.896, Standard Deviation: 0.007\n"
     ]
    }
   ],
   "source": [
    "scores = list()\n",
    "kfold = KFold(10, shuffle=True, random_state=4)\n",
    "\n",
    "for k, (train, test) in enumerate(kfold.split(X2, y)):\n",
    "\t# get data\n",
    "\ttrain_X, test_X = X2[train], X2[test]\n",
    "\ttrain_y, test_y = y[train], y[test]\n",
    "\t# fit model\n",
    "\tmodel = RandomForestClassifier(n_estimators=300)\n",
    "\tmodel.fit(train_X, train_y)\n",
    "\t# evaluate model\n",
    "\typred = model.predict(test_X)\n",
    "\tacc = accuracy_score(test_y, ypred)\n",
    "\t# store score\n",
    "\tscores.append(acc)\n",
    "\tprint(f'fold {k}> accuracy = ', acc)\n",
    "\n",
    "# summarize model performance\n",
    "mean_s, std_s = np.mean(scores), np.std(scores)\n",
    "print('Accuracy Mean: %.3f, Standard Deviation: %.3f' % (mean_s, std_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0> accuracy =  0.9104046242774566\n",
      "fold 1> accuracy =  0.901252408477842\n",
      "fold 2> accuracy =  0.8848747591522158\n",
      "fold 3> accuracy =  0.8978805394990366\n",
      "fold 4> accuracy =  0.899325626204239\n",
      "fold 5> accuracy =  0.9007707129094412\n",
      "fold 6> accuracy =  0.9075144508670521\n",
      "fold 7> accuracy =  0.890655105973025\n",
      "fold 8> accuracy =  0.9118072289156627\n",
      "fold 9> accuracy =  0.9074698795180723\n",
      "Accuracy Mean: 0.901, Standard Deviation: 0.008\n"
     ]
    }
   ],
   "source": [
    "scores = list()\n",
    "kfold = KFold(10, shuffle=True, random_state=4)\n",
    "\n",
    "for k, (train, test) in enumerate(kfold.split(X2, y)):\n",
    "\t# get data\n",
    "\ttrain_X, test_X = X2[train], X2[test]\n",
    "\ttrain_y, test_y = y[train], y[test]\n",
    "\t# fit model\n",
    "\tmodel =  GradientBoostingClassifier()\n",
    "\tmodel.fit(train_X, train_y)\n",
    "\t# evaluate model\n",
    "\typred = model.predict(test_X)\n",
    "\tacc = accuracy_score(test_y, ypred)\n",
    "\t# store score\n",
    "\tscores.append(acc)\n",
    "\tprint(f'fold {k}> accuracy = ', acc)\n",
    "\n",
    "# summarize model performance\n",
    "mean_s, std_s = np.mean(scores), np.std(scores)\n",
    "print('Accuracy Mean: %.3f, Standard Deviation: %.3f' % (mean_s, std_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier()\n",
    "gb_model.fit(X2,y)\n",
    "y_pred = gb_model.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gradient_boosting = pd.DataFrame({'id': id_test, 'NObeyesdad': y_pred})\n",
    "df_gradient_boosting['NObeyesdad']  = df_gradient_boosting['NObeyesdad'].map(nobsedy_invertio)\n",
    "\n",
    "df_gradient_boosting.to_csv('submtions/gradient_boosting2.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não houve uma melhora no treino, pelo contrário, piorou. Subimos pensando inocentemente que teria uma resposta melhor no teste. Então enviamos o Gradient Boosting que já estava tendo resultados melhores e conseguimos 0.89812, e foi melhor mesmo que os outros GradientBoosting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0> accuracy =  0.9104046242774566\n",
      "fold 1> accuracy =  0.903179190751445\n",
      "fold 2> accuracy =  0.8896917148362236\n",
      "fold 3> accuracy =  0.8998073217726397\n",
      "fold 4> accuracy =  0.901252408477842\n",
      "fold 5> accuracy =  0.9046242774566474\n",
      "fold 6> accuracy =  0.9104046242774566\n",
      "fold 7> accuracy =  0.8925818882466281\n",
      "fold 8> accuracy =  0.9151807228915663\n",
      "fold 9> accuracy =  0.9074698795180723\n",
      "Accuracy Mean: 0.903, Standard Deviation: 0.008\n"
     ]
    }
   ],
   "source": [
    "scores = list()\n",
    "kfold = KFold(10, shuffle=True, random_state=4)\n",
    "\n",
    "for k, (train, test) in enumerate(kfold.split(X2, y)):\n",
    "\t# get data\n",
    "\ttrain_X, test_X = X2[train], X2[test]\n",
    "\ttrain_y, test_y = y[train], y[test]\n",
    "\t# fit model\n",
    "\tclf2 = RandomForestClassifier(n_estimators=300, random_state=1)\n",
    "\tclf1 = GradientBoostingClassifier()\n",
    "\tclf3 = GradientBoostingClassifier(n_estimators=300)\n",
    "\tmodel = VotingClassifier( estimators=[('gdb', clf1), ('rf', clf2), ('gdb300', clf3)],  voting='hard')\n",
    "\tmodel.fit(train_X, train_y)\n",
    "\t# evaluate model\n",
    "\typred = model.predict(test_X)\n",
    "\tacc = accuracy_score(test_y, ypred)\n",
    "\t# store score\n",
    "\tscores.append(acc)\n",
    "\tprint(f'fold {k}> accuracy = ', acc)\n",
    "\n",
    "# summarize model performance\n",
    "mean_s, std_s = np.mean(scores), np.std(scores)\n",
    "print('Accuracy Mean: %.3f, Standard Deviation: %.3f' % (mean_s, std_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = RandomForestClassifier(n_estimators=300, random_state=1)\n",
    "clf1 = GradientBoostingClassifier()\n",
    "clf3 = GradientBoostingClassifier(n_estimators=300)\n",
    "model = VotingClassifier( estimators=[('gdb', clf1), ('rf', clf2), ('gdb300', clf3)],  voting='hard')\n",
    "model.fit(X2, y)\n",
    "y_pred = model.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voting = pd.DataFrame({'id': id_test, 'NObeyesdad': y_pred})\n",
    "df_voting['NObeyesdad']  = df_voting['NObeyesdad'].map(nobsedy_invertio)\n",
    "\n",
    "df_voting.to_csv('submtions/voting2.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após os resultados do modelo anterior, subimos esse também, mesmo tendo valores menores no treino. Mas não se saiu melhor que o utlimo algoritmo do cômite de cômite. Conseguimos 0.90065"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
